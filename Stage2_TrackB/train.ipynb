{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23399eda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Underwater Semantic Segmentation using SegFormer\n",
    "Dataset: AI Summit Track B - Underwater Imagery\n",
    "Resolution: 320x256 for consistency\n",
    "Split: 80% train, 20% validation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import SegformerConfig, SegformerForSemanticSegmentation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50271192",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "DATASET_ROOT = \"./rclone-v1.73.0-linux-amd64/TrackB/dataset\"\n",
    "IMG_DIR = os.path.join(DATASET_ROOT, \"images\")\n",
    "MASK_DIR = os.path.join(DATASET_ROOT, \"masks/combined\")\n",
    "\n",
    "# Model & training config\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 320\n",
    "NUM_CLASSES = 8\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 6e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Best model save path\n",
    "BEST_MODEL_PATH = \"./best_segformer_underwater.pth\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Images: {IMG_DIR}\")\n",
    "print(f\"Masks: {MASK_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352d42a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "COLOR_MAP = {\n",
    "    (0, 0, 0): 0,           # Background/Sea-floor\n",
    "    (255, 0, 0): 1,         # Fish\n",
    "    (0, 255, 0): 2,         # Reefs\n",
    "    (0, 0, 255): 3,         # Plants\n",
    "    (255, 255, 0): 4,       # Wrecks\n",
    "    (255, 0, 255): 5,       # Divers\n",
    "    (0, 255, 255): 6,       # Robots\n",
    "    (128, 128, 128): 7      # Others\n",
    "}\n",
    "\n",
    "IDX_TO_COLOR = {v: k for k, v in COLOR_MAP.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feafd01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def rgb_to_label(mask):\n",
    "    \"\"\"Convert RGB mask to integer label mask\"\"\"\n",
    "    mask = np.array(mask)\n",
    "    label = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)\n",
    "\n",
    "    for rgb, idx in COLOR_MAP.items():\n",
    "        matches = np.all(mask == np.array(rgb), axis=-1)\n",
    "        label[matches] = idx\n",
    "\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ec712",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class UnderwaterSegDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, pairs):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, mask_name = self.pairs[idx]\n",
    "\n",
    "        img = Image.open(os.path.join(self.img_dir, img_name)).convert(\"RGB\")\n",
    "        mask = Image.open(os.path.join(self.mask_dir, mask_name)).convert(\"RGB\")\n",
    "\n",
    "        img = img.resize((IMG_WIDTH, IMG_HEIGHT), Image.BILINEAR)\n",
    "        mask = mask.resize((IMG_WIDTH, IMG_HEIGHT), Image.NEAREST)\n",
    "\n",
    "        img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0\n",
    "        mask = torch.from_numpy(rgb_to_label(mask)).long()\n",
    "\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb9576",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_pairs(img_dir, mask_dir):\n",
    "    imgs = sorted([f for f in os.listdir(img_dir)\n",
    "                   if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "    masks = set(os.listdir(mask_dir))\n",
    "\n",
    "    pairs = []\n",
    "    for img in imgs:\n",
    "        base = os.path.splitext(img)[0]\n",
    "        for ext in [\".bmp\", \".png\", \".jpg\"]:\n",
    "            mask_name = base + ext\n",
    "            if mask_name in masks:\n",
    "                pairs.append((img, mask_name))\n",
    "                break\n",
    "\n",
    "    print(f\"Found {len(pairs)} image-mask pairs\")\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ec5fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_pairs = build_pairs(IMG_DIR, MASK_DIR)\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(\n",
    "    all_pairs,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_pairs)}\")\n",
    "print(f\"Val samples: {len(val_pairs)}\")\n",
    "\n",
    "train_ds = UnderwaterSegDataset(IMG_DIR, MASK_DIR, train_pairs)\n",
    "val_ds = UnderwaterSegDataset(IMG_DIR, MASK_DIR, val_pairs)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbbfe9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "config = SegformerConfig.from_pretrained(\n",
    "    \"nvidia/segformer-b2-finetuned-ade-512-512\",\n",
    "    num_labels=NUM_CLASSES\n",
    ")\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b2-finetuned-ade-512-512\",\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Model loaded: SegFormer-B2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222db9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def mean_iou(pred, target, num_classes=NUM_CLASSES):\n",
    "    ious = []\n",
    "    for c in range(num_classes):\n",
    "        p = pred == c\n",
    "        t = target == c\n",
    "        inter = (p & t).sum().item()\n",
    "        union = (p | t).sum().item()\n",
    "        ious.append(1.0 if union == 0 else inter / union)\n",
    "    return np.mean(ious)\n",
    "\n",
    "\n",
    "def mean_f1(pred, target):\n",
    "    return f1_score(\n",
    "        target.cpu().numpy().flatten(),\n",
    "        pred.cpu().numpy().flatten(),\n",
    "        average=\"macro\",\n",
    "        zero_division=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342ce84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for imgs, masks in pbar:\n",
    "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=imgs, labels=masks)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcc223",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    miou_list, f1_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(loader, desc=\"Evaluating\"):\n",
    "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "\n",
    "            outputs = model(pixel_values=imgs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            logits = F.interpolate(\n",
    "                logits,\n",
    "                size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            miou_list.append(mean_iou(preds, masks))\n",
    "            f1_list.append(mean_f1(preds, masks))\n",
    "\n",
    "    return np.mean(miou_list), np.mean(f1_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a47382",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_miou = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING START\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    train_loss = train_one_epoch(train_loader)\n",
    "    val_miou, val_f1 = evaluate(val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val mIoU:   {val_miou:.4f}\")\n",
    "    print(f\"Val F1:     {val_f1:.4f}\")\n",
    "\n",
    "    if val_miou > best_miou:\n",
    "        best_miou = val_miou\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'miou': val_miou,\n",
    "            'f1': val_f1,\n",
    "        }, BEST_MODEL_PATH)\n",
    "        print(f\"âœ“ Best model saved! (mIoU: {val_miou:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"Best Validation mIoU: {best_miou:.4f}\")\n",
    "print(f\"Best model saved at: {BEST_MODEL_PATH}\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
